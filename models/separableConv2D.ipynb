{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa5be75a-ee5f-45b0-891b-da2dd340dd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "f85ed78d-97ef-4979-acac-8d95b34a84ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 21888242871839275222246405745257275088548364400416034343698204186575808495617\n",
    "CIRCOM_PRIME = 21888242871839275222246405745257275088548364400416034343698204186575808495617\n",
    "MAX_POSITIVE = CIRCOM_PRIME // 2\n",
    "MAX_NEGATIVE = MAX_POSITIVE + 1 # The most positive number\n",
    "CIRCOM_NEGATIVE_1 = 21888242871839275222246405745257275088548364400416034343698204186575808495617 - 1\n",
    "EXPONENT = 15\n",
    "\n",
    "class SeparableConv2D(nn.Module):\n",
    "    '''Separable convolution'''\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(SeparableConv2D, self).__init__()\n",
    "        self.dw_conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=stride, padding=1, groups=in_channels, bias=False)\n",
    "        self.pw_conv =  nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dw_conv(x)\n",
    "        x = self.pw_conv(x)\n",
    "        return x\n",
    "\n",
    "def from_circom(x):\n",
    "    if type(x) != int:\n",
    "        x = int(x)\n",
    "    if x > MAX_POSITIVE: \n",
    "        return x - CIRCOM_PRIME\n",
    "    return x\n",
    "    \n",
    "def to_circom(x):\n",
    "    if type(x) != int:\n",
    "        x = int(x)\n",
    "    if x < 0:\n",
    "        return x + CIRCOM_PRIME \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "32e117a1-32aa-4b4b-91d0-4b4b78071b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DepthwiseConv(nRows, nCols, nChannels, nFilters, kernelSize, strides, n, input, weights, bias):\n",
    "    assert(nFilters % nChannels == 0)\n",
    "    outRows = (nRows - kernelSize)//strides + 1\n",
    "    outCols = (nCols - kernelSize)//strides + 1\n",
    "    \n",
    "    out = np.zeros((outRows, outCols, nFilters))\n",
    "    remainder = np.zeros((outRows, outCols, nFilters))\n",
    "    \n",
    "    for row in range(outRows):\n",
    "        for col in range(outCols):\n",
    "            for channel in range(nChannels):\n",
    "                for x in range(kernelSize):\n",
    "                    for y in range(kernelSize):\n",
    "                        out[row, col, channel] += input[row*strides+x, col*strides+y, channel] * weights[x, y, channel]\n",
    "                \n",
    "                out[row][col][channel] += bias[channel]\n",
    "                remainder[row][col][channel] = out[row][col][channel] % n\n",
    "                out[row][col][channel] = out[row][col][channel] / n\n",
    "                            \n",
    "    return out, remainder\n",
    "    \n",
    "def PointwiseConv2d(nRows, nCols, nChannels, nFilters, strides, n, input, weights, bias):\n",
    "    kernelSize = 1\n",
    "    outRows = (nRows - kernelSize)//strides + 1\n",
    "    outCols = (nCols - kernelSize)//strides + 1\n",
    "    out = np.zeros((outRows, outCols, nFilters))\n",
    "    for row in range(outRows):\n",
    "        for col in range(outCols):\n",
    "            for filter in range(nFilters):\n",
    "                for k in range(nChannels):\n",
    "                    out[row, col, filter] += input[row*strides, col*strides, k] * weights[k, filter]\n",
    "                    \n",
    "                out[row][col][filter] += bias[filter]\n",
    "                out[row][col][filter] = out[row][col][filter] / n\n",
    "                            \n",
    "    return out\n",
    "\n",
    "def SeparableConvImpl(nRows, nCols, nChannels, nDepthFilters, nPointFilters, kernelSize, strides, n, input, depthWeights, pointWeights, depthBias, pointBias):\n",
    "    outRows = (nRows - kernelSize)//strides + 1\n",
    "    outCols = (nCols - kernelSize)//strides + 1\n",
    "\n",
    "    depthOut, rem = DepthwiseConv(nRows, nCols, nChannels, nDepthFilters, kernelSize, strides, n, input, depthWeights, depthBias)\n",
    "    pointOut = PointwiseConv2d(outRows, outCols, nChannels, nPointFilters, strides, n, depthOut, pointWeights, pointBias)\n",
    "    return pointOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "e3dfadfd-2587-4708-9dd5-535a999ed359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DepthwiseConvInt(nRows, nCols, nChannels, nFilters, kernelSize, strides, n, input, weights, bias):\n",
    "    assert(nFilters % nChannels == 0)\n",
    "    outRows = (nRows - kernelSize)//strides + 1\n",
    "    outCols = (nCols - kernelSize)//strides + 1\n",
    "    \n",
    "    Input = [[[str(input[i][j][k] % p) for k in range(nChannels)] for j in range(nCols)] for i in range(nRows)]\n",
    "    Weights = [[[str(weights[i][j][k] % p) for k in range(nChannels)] for j in range(kernelSize)] for i in range(kernelSize)]\n",
    "    Bias = [str(bias[i] % p) for i in range(nFilters)]\n",
    "    out = [[[0 for _ in range(nFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    str_out = [[[0 for _ in range(nFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    remainder = [[[None for _ in range(nFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    \n",
    "    for row in range(outRows):\n",
    "        for col in range(outCols):\n",
    "            for channel in range(nChannels):\n",
    "                for x in range(kernelSize):\n",
    "                    for y in range(kernelSize):\n",
    "                        out[row][col][channel] += int(input[row*strides+x][col*strides+y][channel]) * int(weights[x][y][channel])\n",
    "                        \n",
    "                out[row][col][channel] += int(bias[channel])\n",
    "                remainder[row][col][channel] = str(int(out[row][col][channel] % n))\n",
    "                out[row][col][channel] = int((out[row][col][channel] // n))\n",
    "                str_out[row][col][channel] = str(out[row][col][channel] % p)\n",
    "                \n",
    "    return Input, Weights, Bias, out, str_out, remainder\n",
    "    \n",
    "def PointwiseConv2dInt(nRows, nCols, nChannels, nFilters, strides, n, input, weights, bias):\n",
    "    kernelSize = 1\n",
    "    outRows = (nRows - kernelSize)//strides + 1\n",
    "    outCols = (nCols - kernelSize)//strides + 1\n",
    "    \n",
    "    Input = [[[str(input[i][j][k] % p) for k in range(nChannels)] for j in range(nCols)] for i in range(nRows)]\n",
    "    Weights = [[str(weights[k][l] % p) for l in range(nFilters)] for k in range(nChannels)]\n",
    "    Bias = [str(bias[i] % p) for i in range(nFilters)]\n",
    "    out = [[[0 for _ in range(nFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    str_out = [[[0 for _ in range(nFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    remainder = [[[None for _ in range(nFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    \n",
    "    for row in range(outRows):\n",
    "        for col in range(outCols):\n",
    "            for filter in range(nFilters):\n",
    "                for channel in range(nChannels):\n",
    "                    out[row][col][filter] += int(input[row*strides][col*strides][channel]) * int(weights[channel][filter])\n",
    "                            \n",
    "                out[row][col][filter] += int(bias[filter])\n",
    "                remainder[row][col][filter] = str(int(out[row][col][filter] % n))\n",
    "                out[row][col][filter] = str(out[row][col][filter] // n % p)\n",
    "    return Input, Weights, Bias, out, remainder\n",
    "\n",
    "def SeparableConvInt(nRows, nCols, nChannels, nDepthFilters, nPointFilters, kernelSize, strides, n, input, depthWeights, pointWeights, depthBias, pointBias):\n",
    "    outRows = (nRows - kernelSize)//strides + 1\n",
    "    outCols = (nCols - kernelSize)//strides + 1\n",
    "\n",
    "    Input, DepthWeights, DepthBias, depthOut, depthStrOut, depthRemainder = DepthwiseConvInt(nRows, nCols, nChannels, nDepthFilters, kernelSize, strides, n, input, depthWeights, depthBias)\n",
    "    test = [[[from_circom(int(depthStrOut[i][j][k])) for k in range(3)] for j in range(5)] for i in range(5)]\n",
    "    assert(test == depthOut)\n",
    "    pInput, PointWeights, PointBias, pointOut, pointRem = PointwiseConv2dInt(outRows, outCols, nChannels, nPointFilters, strides, n, depthOut, pointWeights, pointBias)\n",
    "    \n",
    "    return Input, DepthWeights, DepthBias, depthOut, depthStrOut, depthRemainder, PointWeights, PointBias, pointOut, pointRem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "cfa6df6a-9a0c-4150-9df0-30e2700d4f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn((1, 3, 5, 5))\n",
    "model = SeparableConv2D(3, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "5db59f3a-e610-473a-8dfc-1d5e812748f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.dw_conv.weight.squeeze().detach().numpy()\n",
    "bias = torch.zeros(weights.shape[0]).numpy()\n",
    "# input = torch.randn((1, 8, 32, 32))\n",
    "\n",
    "expected = model.dw_conv(input).detach().numpy()\n",
    "\n",
    "# # Converting to H x W x C\n",
    "padded = F.pad(input, (1,1,1,1), \"constant\", 0) # Padding for convolution with \"same\" configuration\n",
    "padded = padded.squeeze().numpy().transpose((1, 2, 0))\n",
    "weights = weights.transpose((1, 2, 0))\n",
    "\n",
    "actual, rem = DepthwiseConv(7, 7, 3, 3, 3, 1, 1, padded, weights, bias)\n",
    "expected = expected.squeeze().transpose((1, 2, 0))\n",
    "\n",
    "assert(np.allclose(expected, actual, atol=0.00001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "47ce42c7-b417-46ba-a31b-1ae23234a2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights.shape=(6, 3, 1, 1)\n",
      "(5, 5, 3)\n"
     ]
    }
   ],
   "source": [
    "weights = model.pw_conv.weight.detach().numpy()\n",
    "print(f\"{weights.shape=}\")\n",
    "bias = torch.zeros(weights.shape[0]).numpy()\n",
    "\n",
    "expected = model.pw_conv(input).detach().numpy()\n",
    "\n",
    "# # Converting to H x W x C\n",
    "padded = input.squeeze().numpy().transpose((1, 2, 0))\n",
    "print(padded.shape)\n",
    "weights = weights.transpose((2, 3, 1, 0)).squeeze()\n",
    "\n",
    "actual = PointwiseConv2d(5, 5, 3, 6, 1, 1, padded, weights, bias)\n",
    "\n",
    "expected = expected.squeeze().transpose((1, 2, 0))\n",
    "\n",
    "assert(np.allclose(expected, actual, atol=0.00001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "7a36a0a1-6965-4184-93a5-5d121d6d1856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights.shape=(3, 6)\n",
      "expected.shape=(1, 6, 5, 5)\n",
      "(6,)\n"
     ]
    }
   ],
   "source": [
    "depthWeights = model.dw_conv.weight.squeeze().detach().numpy()\n",
    "depthBias = torch.zeros(weights.shape[0]).numpy()\n",
    "# input = torch.randn((1, 8, 32, 32))\n",
    "depthWeights = depthWeights.transpose((1, 2, 0))\n",
    "\n",
    "pointWeights = model.pw_conv.weight.detach().numpy()\n",
    "print(f\"{weights.shape=}\")\n",
    "pointBias = torch.zeros(pointWeights.shape[0]).numpy()\n",
    "pointWeights = pointWeights.transpose((2, 3, 1, 0)).squeeze()\n",
    "\n",
    "expected = model(input).detach().numpy()\n",
    "print(f\"{expected.shape=}\")\n",
    "# pointInput = model.dw_conv(input)\n",
    "# expected = model.pw_conv(pointInput).detach().numpy()\n",
    "\n",
    "# # Converting to H x W x C\n",
    "padded = F.pad(input, (1,1,1,1), \"constant\", 0) # Padding for convolution with \"same\" configuration\n",
    "padded = padded.squeeze().numpy().transpose((1, 2, 0))\n",
    "\n",
    "print(pointBias.shape)\n",
    "# SeparableConvImpl(nRows, nCols, nChannels, nDepthFilters, nPointFilters, kernelSize, strides, n, input, depthWeights, pointWeights, depthBias, pointBias):\n",
    "actual = SeparableConvImpl(7, 7, 3, 3, 6, 3, 1, 1, padded, depthWeights, pointWeights, depthBias, pointBias)\n",
    "expected = expected.squeeze().transpose((1, 2, 0))\n",
    "\n",
    "assert(np.allclose(expected, actual, atol=0.00001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "c65773ab-e39b-4bfe-9b99-987894d06594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights.shape=(3, 6)\n",
      "expected.shape=(1, 6, 5, 5)\n",
      "(6,)\n",
      "weights.shape=(3, 6)\n",
      "expected.shape=(1, 6, 5, 5)\n",
      "(6,)\n",
      "actual[0][0][0]=17143474868210.0\n",
      "expected[0][0][0]=0.017143477\n",
      "dw_expected.shape=torch.Size([1, 3, 5, 5])\n",
      "dw_expected.shape=(5, 5, 3)\n",
      "-0.40900424\n",
      "-409004260094254.0\n",
      "to_circom(-1695193072.0)=21888242871839275222246405745257275088548364400416034343698204186574113302545\n",
      "to_circom(-1695193072)=21888242871839275222246405745257275088548364400416034343698204186574113302545\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-169519305504806928"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depthWeights = model.dw_conv.weight.squeeze().detach().numpy()\n",
    "depthBias = torch.zeros(weights.shape[0]).numpy()\n",
    "# input = torch.randn((1, 8, 32, 32))\n",
    "depthWeights = depthWeights.transpose((1, 2, 0))\n",
    "\n",
    "pointWeights = model.pw_conv.weight.detach().numpy()\n",
    "print(f\"{weights.shape=}\")\n",
    "pointBias = torch.zeros(pointWeights.shape[0]).numpy()\n",
    "pointWeights = pointWeights.transpose((2, 3, 1, 0)).squeeze()\n",
    "\n",
    "expected = model(input).detach().numpy()\n",
    "print(f\"{expected.shape=}\")\n",
    "# pointInput = model.dw_conv(input)\n",
    "# expected = model.pw_conv(pointInput).detach().numpy()\n",
    "\n",
    "# # Converting to H x W x C\n",
    "padded = F.pad(input, (1,1,1,1), \"constant\", 0) # Padding for convolution with \"same\" configuration\n",
    "padded = padded.squeeze().numpy().transpose((1, 2, 0))\n",
    "\n",
    "print(pointBias.shape)\n",
    "# SeparableConvImpl(nRows, nCols, nChannels, nDepthFilters, nPointFilters, kernelSize, strides, n, input, depthWeights, pointWeights, depthBias, pointBias):\n",
    "actual = SeparableConvImpl(7, 7, 3, 3, 6, 3, 1, 1, padded, depthWeights, pointWeights, depthBias, pointBias)\n",
    "expected = expected.squeeze().transpose((1, 2, 0))\n",
    "\n",
    "assert(np.allclose(expected, actual))\n",
    "\n",
    "def SeparableConvTesting(nRows, nCols, nChannels, nDepthFilters, nPointFilters, kernelSize, strides, n, input, depthWeights, pointWeights, depthBias, pointBias):\n",
    "    assert(nDepthFilters % nChannels == 0)\n",
    "    outRows = (nRows - kernelSize)//strides + 1\n",
    "    outCols = (nCols - kernelSize)//strides + 1\n",
    "    \n",
    "    depthOut = np.zeros((outRows, outCols, nDepthFilters))\n",
    "    depthRemainder = np.zeros((outRows, outCols, nDepthFilters))\n",
    "\n",
    "    # Depthwise convolution\n",
    "    for row in range(outRows):\n",
    "        for col in range(outCols):\n",
    "            for channel in range(nChannels):\n",
    "                for x in range(kernelSize):\n",
    "                    for y in range(kernelSize):\n",
    "                        # depthOut[row, col, channel] += input[row*strides+x, col*strides+y, channel] * depthWeights[x, y, channel]\n",
    "                        depthOut[row][col][channel] += int(input[row*strides+x, col*strides+y, channel]) * int(depthWeights[x, y, channel])\n",
    "                \n",
    "                depthOut[row][col][channel] += depthBias[channel]\n",
    "                depthRemainder[row][col][channel] = depthOut[row][col][channel] % n\n",
    "                depthOut[row][col][channel] = depthOut[row][col][channel] // n\n",
    "                \n",
    "    # Pointwise convolution\n",
    "    out = np.zeros((outRows, outCols, nPointFilters))\n",
    "    remainder = np.zeros((outRows, outCols, nPointFilters))\n",
    "    \n",
    "    for row in range(outRows):\n",
    "        for col in range(outCols):\n",
    "            for filter in range(nPointFilters):\n",
    "                for k in range(nDepthFilters):\n",
    "                    out[row, col, filter] += depthOut[row*strides, col*strides, k] * pointWeights[k, filter]\n",
    "                    \n",
    "                out[row][col][filter] += pointBias[filter]\n",
    "                out[row][col][filter] = out[row][col][filter] // n\n",
    "                \n",
    "    return out, remainder, depthOut, depthRemainder\n",
    "    \n",
    "depthWeights = model.dw_conv.weight.squeeze().detach().numpy()\n",
    "depthBias = torch.zeros(weights.shape[0], dtype=int).numpy()\n",
    "# input = torch.randn((1, 8, 32, 32))\n",
    "depthWeights = depthWeights.transpose((1, 2, 0))\n",
    "\n",
    "pointWeights = model.pw_conv.weight.detach().numpy()\n",
    "print(f\"{weights.shape=}\")\n",
    "pointBias = torch.zeros(pointWeights.shape[0]).numpy()\n",
    "pointWeights = pointWeights.transpose((2, 3, 1, 0)).squeeze()\n",
    "\n",
    "expected = model(input).detach().numpy()\n",
    "print(f\"{expected.shape=}\")\n",
    "# pointInput = model.dw_conv(input)\n",
    "# expected = model.pw_conv(pointInput).detach().numpy()\n",
    "\n",
    "# # Converting to H x W x C\n",
    "padded = F.pad(input, (1,1,1,1), \"constant\", 0) # Padding for convolution with \"same\" configuration\n",
    "padded = padded.squeeze().numpy().transpose((1, 2, 0))\n",
    "\n",
    "print(pointBias.shape)\n",
    "# SeparableConvImpl(nRows, nCols, nChannels, nDepthFilters, nPointFilters, kernelSize, strides, n, input, depthWeights, pointWeights, depthBias, pointBias):\n",
    "\n",
    "quantized_image = padded * 10**EXPONENT\n",
    "quantized_depth_weights = depthWeights * 10**EXPONENT\n",
    "quantized_point_weights = pointWeights * 10**EXPONENT\n",
    "\n",
    "actual, _, depthOut, depthRemainder = SeparableConvTesting(7, 7, 3, 3, 6, 3, 1, 10**EXPONENT, quantized_image.round().astype(int), quantized_depth_weights.round().astype(int), quantized_point_weights.round().astype(int), depthBias, pointBias)\n",
    "expected = expected.squeeze().transpose((1, 2, 0))\n",
    "\n",
    "print(f\"{actual[0][0][0]=}\")\n",
    "print(f\"{expected[0][0][0]=}\")\n",
    "\n",
    "actual = actual / 10**EXPONENT\n",
    "assertDepthOut = depthOut / 10**EXPONENT\n",
    "\n",
    "dw_expected = model.dw_conv(input)\n",
    "pw_expected = model.pw_conv(dw_expected).detach().numpy()\n",
    "print(f\"{dw_expected.shape=}\")\n",
    "dw_expected = dw_expected.detach().numpy().squeeze().transpose((1, 2, 0))\n",
    "print(f\"{dw_expected.shape=}\")\n",
    "pw_expected = pw_expected.squeeze().transpose((1, 2, 0))\n",
    "\n",
    "assert(np.allclose(expected, actual))\n",
    "assert(np.allclose(expected, pw_expected))\n",
    "assert(np.allclose(assertDepthOut, dw_expected))\n",
    "\n",
    "input_json_path = \"depthwiseConv2D_input_test.json\"\n",
    "with open(input_json_path, \"w\") as input_file:\n",
    "    json.dump({\"in\": quantized_image.round().astype(int).tolist(),\n",
    "               \"weights\": quantized_depth_weights.round().astype(int).tolist(),\n",
    "               \"remainder\": depthRemainder.round().astype(int).tolist(),\n",
    "               \"out\": depthOut.round().astype(int).tolist(),\n",
    "               \"bias\": depthBias.round().astype(int).tolist(),\n",
    "              },\n",
    "              input_file)\n",
    "\n",
    "\n",
    "print(dw_expected[0][0][0])\n",
    "print(depthOut.tolist()[0][0][0])\n",
    "print(f\"{to_circom(-1695193072.0)=}\")\n",
    "print(f\"{to_circom(-1695193072)=}\")\n",
    "\n",
    "print(to_circom(-1695193072) == 21888242871839275222246405745257275088548364400416034343698204186574113302545)\n",
    "to_circom(-1695193072 * 10**EXPONENT) - 21888242871839275222246405745257275088548364400416034343698034667268608495617\n",
    "21888242871839275222246405745257275088548364400416034343698034667268608495617 - 21888242871839275222246405745257275088548364400416034343698204186574113302545\n",
    "# ==from_circom(21888242871839275222246405745257275088548364400416034343698204186574113302545)=}\")\n",
    "# print(f\"{from_circom(21888242871839275222246405745257275088548364400416034343681252255859160055228)=}\")\n",
    "# print(f\"{from_circom(21888242871839275222246405745257275088548364400416034343698034667271960057857)=}\")\n",
    "# print(f\"{from_circom(21888242871839275222246405745257275088548364400416034343681252255859160055228)=}\")\n",
    "# input_json_path = \"separableConv2D_input.json\"\n",
    "# with open(input_json_path, \"w\") as input_file:\n",
    "#     json.dump({\"in\": Input,\n",
    "#                \"depthWeights\": DepthWeights,\n",
    "#                \"depthBias\": DepthBias,\n",
    "#                \"depthRemainder\": DepthRemainder,\n",
    "#                \"depthOut\": DepthOut,\n",
    "               \n",
    "#                \"pointWeights\": PointWeights,\n",
    "#                \"pointBias\": PointBias,\n",
    "#                \"pointRemainder\": remainder,\n",
    "#                \"pointOut\": out,\n",
    "#               },\n",
    "#               input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "fd587b58-1646-4ec8-9a79-4f891dab0276",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded = F.pad(input, (1,1,1,1), \"constant\", 0)\n",
    "padded = padded.squeeze().numpy().transpose((1, 2, 0))\n",
    "# weights = weights.transpose((1, 2, 0))\n",
    "\n",
    "quantized_image = padded * 10**EXPONENT\n",
    "quantized_depth_weights = depthWeights * 10**EXPONENT\n",
    "quantized_point_weights = pointWeights * 10**EXPONENT\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# q_input, q_weights, bias, actual, str_actual, rem = DepthwiseConvInt(7, 7, 3, 3, 3, 1, 10**EXPONENT, quantized_image.round().astype(int), quantized_depth_weights.round().astype(int), depthBias.astype(int))\n",
    "\n",
    "# Input, DepthWeights, DepthBias, DepthOut, depthStrOut, depthRemainder, _, _, _, _ = SeparableConvTest(7, 7, 3, 3, 6, 3, 1, 10**EXPONENT, quantized_image.round().astype(int), quantized_depth_weights.round().astype(int), quantized_point_weights.round().astype(int), depthBias.astype(int), pointBias.astype(int))\n",
    "Input, DepthWeights, DepthBias, depthOut, depthStrOut, DepthRem, PointWeights, PointBias, pointOut, pointRem = SeparableConvTest(7, 7, 3, 3, 6, 3, 1, 10**EXPONENT, quantized_image.round().astype(int), quantized_depth_weights.round().astype(int), quantized_point_weights.round().astype(int), depthBias.astype(int), pointBias.astype(int))\n",
    "# Input, DepthWeights, DepthBias, depthOut, depthStrOut, depthRemainder, PointWeights, PointBias, remainder, out \n",
    "\n",
    "input_json_path = \"separableConv2D_input.json\"\n",
    "with open(input_json_path, \"w\") as input_file:\n",
    "    json.dump({\"in\": Input,\n",
    "               \"depthWeights\": DepthWeights,\n",
    "               \"depthBias\": DepthBias,\n",
    "               \"depthRemainder\": DepthRem,\n",
    "               \"depthOut\": depthStrOut,\n",
    "               \n",
    "               \"pointWeights\": PointWeights,\n",
    "               \"pointBias\": PointBias,\n",
    "               \"pointRemainder\": pointRem,\n",
    "               \"pointOut\": pointOut,\n",
    "              },\n",
    "              input_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eec715f-be89-4e42-8656-8356a31f4aac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9f310a6e-0009-4361-b4ca-f736a6c71a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights.shape=(3, 6)\n",
      "expected.shape=(5, 5, 6)\n",
      "(6,)\n",
      "depthOut[0][0][0]=-1.0\n",
      "dw_expected[0][0][0]=-0.1695193\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[125], line 83\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdepthOut[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdw_expected[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(np\u001b[38;5;241m.\u001b[39mallclose(dw_expected, depthOut))\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# quantized_image = padded * 10**EXPONENT\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# quantized_depth_weights = depthWeights * 10**EXPONENT\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# quantized_point_weights = pointWeights * 10**EXPONENT\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     90\u001b[0m \n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# assert(np.allclose(expected, actual))\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def SeparableConvTest(nRows, nCols, nChannels, nDepthFilters, nPointFilters, kernelSize, strides, n, input, depthWeights, pointWeights, depthBias, pointBias):\n",
    "    assert(nDepthFilters % nChannels == 0)\n",
    "    outRows = (nRows - kernelSize)//strides + 1\n",
    "    outCols = (nCols - kernelSize)//strides + 1\n",
    "    \n",
    "    depthOut = np.zeros((outRows, outCols, nDepthFilters))\n",
    "    depthRemainder = np.zeros((outRows, outCols, nDepthFilters))\n",
    "\n",
    "    # Depthwise convolution\n",
    "    for row in range(outRows):\n",
    "        for col in range(outCols):\n",
    "            for channel in range(nChannels):\n",
    "                for x in range(kernelSize):\n",
    "                    for y in range(kernelSize):\n",
    "                        depthOut[row, col, channel] += input[row*strides+x, col*strides+y, channel] * depthWeights[x, y, channel]\n",
    "                \n",
    "                depthOut[row][col][channel] += depthBias[channel]\n",
    "                depthRemainder[row][col][channel] = depthOut[row][col][channel] % n\n",
    "                depthOut[row][col][channel] = depthOut[row][col][channel] // n\n",
    "                            \n",
    "    # Pointwise convolution\n",
    "    out = np.zeros((outRows, outCols, nPointFilters))\n",
    "    remainder = np.zeros((outRows, outCols, nPointFilters))\n",
    "    \n",
    "    for row in range(outRows):\n",
    "        for col in range(outCols):\n",
    "            for filter in range(nPointFilters):\n",
    "                for k in range(nDepthFilters):\n",
    "                    out[row, col, filter] += depthOut[row*strides, col*strides, k] * pointWeights[k, filter]\n",
    "                    \n",
    "                out[row][col][filter] += pointBias[filter]\n",
    "                out[row][col][filter] = out[row][col][filter] // n\n",
    "                \n",
    "    return depthOut, depthRemainder, out\n",
    "    \n",
    "# quantized_image = padded * 10**EXPONENT\n",
    "# quantized_depth_weights = depthWeights * 10**EXPONENT\n",
    "# quantized_point_weights = pointWeights * 10**EXPONENT\n",
    "    \n",
    "# DepthOut, remainder, out = SeparableConvTest(7, 7, 3, 3, 6, 3, 1, 10**EXPONENT, quantized_image.round().astype(int), quantized_depth_weights.round().astype(int), quantized_point_weights.round().astype(int), depthBias.astype(int), pointBias.astype(int))\n",
    "# # q_input, q_weights, bias, str_actual, actual, rem = DepthwiseConvInt(7, 7, 3, 3, 3, 1, 10**EXPONENT, quantized_image.round().astype(int), quantized_depth_weights.round().astype(int), depthBias.astype(int))\n",
    "# dw_expected = model.dw_conv(input)\n",
    "# pw_expected = model.dw_conv(dw_expected)\n",
    "# dw_expected = dw_expected.detach().numpy()\n",
    "# pw_expected = pw_expected.detach().numpy()\n",
    "# DepthOut = DepthOut / 10**EXPONENT\n",
    "# dw_expected = dw_expected.squeeze().transpose((1, 2, 0))\n",
    "# pw_expected = pw_expected.squeeze().transpose((1, 2, 0))\n",
    "# print(DepthOut[0][0][0])\n",
    "# print(dw_expected[0][0][0])\n",
    "# print(out[0][0][0])\n",
    "# print(pw_expected[0][0][0])\n",
    "\n",
    "# assert(np.allclose(DepthOut, dw_expected))\n",
    "\n",
    "depthWeights = model.dw_conv.weight.squeeze().detach().numpy()\n",
    "depthBias = torch.zeros(weights.shape[0]).numpy()\n",
    "# input = torch.randn((1, 8, 32, 32))\n",
    "depthWeights = depthWeights.transpose((1, 2, 0))\n",
    "\n",
    "pointWeights = model.pw_conv.weight.detach().numpy()\n",
    "print(f\"{weights.shape=}\")\n",
    "pointBias = torch.zeros(pointWeights.shape[0]).numpy()\n",
    "pointWeights = pointWeights.transpose((2, 3, 1, 0)).squeeze()\n",
    "\n",
    "print(f\"{expected.shape=}\")\n",
    "# pointInput = model.dw_conv(input)\n",
    "# expected = model.pw_conv(pointInput).detach().numpy()\n",
    "\n",
    "# # Converting to H x W x C\n",
    "padded = F.pad(input, (1,1,1,1), \"constant\", 0) # Padding for convolution with \"same\" configuration\n",
    "padded = padded.squeeze().numpy().transpose((1, 2, 0))\n",
    "\n",
    "print(pointBias.shape)\n",
    "# SeparableConvImpl(nRows, nCols, nChannels, nDepthFilters, nPointFilters, kernelSize, strides, n, input, depthWeights, pointWeights, depthBias, pointBias):\n",
    "depthOut, _, _ = SeparableConvTest(7, 7, 3, 3, 6, 3, 1, 10**EXPONENT, padded, depthWeights, pointWeights, depthBias, pointBias)\n",
    "dw_expected = model.dw_conv(input).detach().numpy()\n",
    "# expected = model(input).detach().numpy()\n",
    "dw_expected = dw_expected.squeeze().transpose((1, 2, 0))\n",
    "\n",
    "print(f\"{depthOut[0][0][0]=}\")\n",
    "print(f\"{dw_expected[0][0][0]=}\")\n",
    "assert(np.allclose(dw_expected, depthOut))\n",
    "\n",
    "# quantized_image = padded * 10**EXPONENT\n",
    "# quantized_depth_weights = depthWeights * 10**EXPONENT\n",
    "# quantized_point_weights = pointWeights * 10**EXPONENT\n",
    "\n",
    "# depthOut, _, _ = SeparableConvTest(7, 7, 3, 3, 6, 3, 1, 1, padded, quantized_image.round().astype(int), pointWeights, depthBias, pointBias)\n",
    "\n",
    "# assert(np.allclose(expected, actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "368fddb0-87ef-463a-8375-76fbae2c112c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def SeparableConvTest(nRows, nCols, nChannels, nDepthFilters, nPointFilters, kernelSize, strides, n, input, depthWeights, pointWeights, depthBias, pointBias):\n",
    "#     assert(nDepthFilters % nChannels == 0)\n",
    "#     outRows = (nRows - kernelSize)//strides + 1\n",
    "#     outCols = (nCols - kernelSize)//strides + 1\n",
    "    \n",
    "#     depthOut = np.zeros((outRows, outCols, nDepthFilters))\n",
    "#     depthRemainder = np.zeros((outRows, outCols, nDepthFilters))\n",
    "\n",
    "#     # Depthwise convolution\n",
    "#     for row in range(outRows):\n",
    "#         for col in range(outCols):\n",
    "#             for channel in range(nChannels):\n",
    "#                 for x in range(kernelSize):\n",
    "#                     for y in range(kernelSize):\n",
    "#                         depthOut[row, col, channel] += input[row*strides+x, col*strides+y, channel] * depthWeights[x, y, channel]\n",
    "                \n",
    "#                 depthOut[row][col][channel] += depthBias[channel]\n",
    "#                 depthRemainder[row][col][channel] = depthOut[row][col][channel] % n\n",
    "#                 depthOut[row][col][channel] = depthOut[row][col][channel] / n\n",
    "                            \n",
    "#     # Pointwise convolution\n",
    "#     out = np.zeros((outRows, outCols, nPointFilters))\n",
    "#     remainder = np.zeros((outRows, outCols, nPointFilters))\n",
    "    \n",
    "#     for row in range(outRows):\n",
    "#         for col in range(outCols):\n",
    "#             for filter in range(nPointFilters):\n",
    "#                 for k in range(nDepthFilters):\n",
    "#                     out[row, col, filter] += depthOut[row*strides, col*strides, k] * pointWeights[k, filter]\n",
    "                    \n",
    "#                 out[row][col][filter] += pointBias[filter]\n",
    "#                 out[row][col][filter] = out[row][col][filter] / n\n",
    "                \n",
    "#     return out, remainder\n",
    "\n",
    "# def SeparableConv2DInt(nRows, nCols, nChannels, nFilters, kernelSize, strides, n, input, weights, bias):\n",
    "def SeparableConvTest(nRows, nCols, nChannels, nDepthFilters, nPointFilters, kernelSize, strides, n, input, depthWeights, pointWeights, depthBias, pointBias):\n",
    "    outRows = (nRows - kernelSize)//strides + 1\n",
    "    outCols = (nCols - kernelSize)//strides + 1\n",
    "    \n",
    "    Input = [[[str(input[i][j][k] % p) for k in range(nChannels)] for j in range(nCols)] for i in range(nRows)]\n",
    "    DepthWeights = [[[str(depthWeights[i][j][l] % p) for l in range(nDepthFilters)] for j in range(kernelSize)] for i in range(kernelSize)]\n",
    "    DepthBias = [str(int(bias[i] % p)) for i in range(nDepthFilters)]\n",
    "    depthOut = [[[0 for _ in range(nDepthFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    depthStrOut = [[[0 for _ in range(nDepthFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    depthRemainder = [[[None for _ in range(nDepthFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    \n",
    "    for row in range(outRows):\n",
    "        for col in range(outCols):\n",
    "            for channel in range(nChannels):\n",
    "                for x in range(kernelSize):\n",
    "                    for y in range(kernelSize):\n",
    "                        depthOut[row][col][channel] += input[row*strides+x, col*strides+y, channel] * depthWeights[x, y, channel]\n",
    "                \n",
    "                # remainder[row][col][filter] = str(out[row][col][filter] % n)\n",
    "                # out[row][col][filter] = str(out[row][col][filter] // n % p)\n",
    "                \n",
    "                depthOut[row][col][channel] += depthBias[channel]\n",
    "                depthRemainder[row][col][channel] = str(depthOut[row][col][channel] % n)\n",
    "                depthOut[row][col][channel] = depthOut[row][col][channel] // n % p\n",
    "                depthStrOut[row][col][channel] = str(depthOut[row][col][channel])\n",
    "                \n",
    "    PointWeights = [[[str(depthWeights[i][j][l] % p) for l in range(nDepthFilters)] for j in range(kernelSize)] for i in range(kernelSize)]\n",
    "    PointBias = [str(bias[i] % p) for i in range(nDepthFilters)]\n",
    "    PointOut = [[[0 for _ in range(nDepthFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    PointStrOut = [[[0 for _ in range(nDepthFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    PointRemainder = [[[None for _ in range(nDepthFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    \n",
    "    PointWeights = [[str(pointWeights[k][l] % p) for l in range(nPointFilters)] for k in range(nDepthFilters)]\n",
    "    PointBias = [str(int(bias[i] % p)) for i in range(nPointFilters)]\n",
    "    out = [[[0 for _ in range(nPointFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    remainder = [[[None for _ in range(nPointFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    \n",
    "    for row in range(outRows):\n",
    "        for col in range(outCols):\n",
    "            for filter in range(nPointFilters):\n",
    "                for channel in range(nDepthFilters):\n",
    "                    # out[row][col][filter] += input[row*strides][col*strides][channel] * weights[channel][filter]\n",
    "                        # print(type(depthOut[row*strides][col*strides][channel]))\n",
    "                        # print(type(pointWeights[channel][filter].astype(int)))\n",
    "                        # print(type(depthOut[row*strides][col*strides][channel] * pointWeights[channel][filter].astype(int)))\n",
    "                        out[row][col][filter] += int(depthOut[row*strides][col*strides][channel]) * int(pointWeights[channel][filter].astype(int))\n",
    "                            \n",
    "                out[row][col][filter] += pointBias[filter]\n",
    "                remainder[row][col][filter] = str(out[row][col][filter] % n)\n",
    "                out[row][col][filter] = str(out[row][col][filter] // n % p)\n",
    "                \n",
    "    return Input, DepthWeights, DepthBias, depthRemainder, depthStrOut, PointWeights, PointBias, remainder, out \n",
    "    # return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0f5b3824-e996-4971-8cca-3a51a9021991",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_46135/3291142876.py:42: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  out[row][col][filter] += input[row*strides][col*strides][channel] * weights[channel][filter]\n",
      "/tmp/ipykernel_46135/3291142876.py:42: RuntimeWarning: overflow encountered in scalar add\n",
      "  out[row][col][filter] += input[row*strides][col*strides][channel] * weights[channel][filter]\n"
     ]
    }
   ],
   "source": [
    "padded = F.pad(input, (1,1,1,1), \"constant\", 0)\n",
    "padded = padded.squeeze().numpy().transpose((1, 2, 0))\n",
    "# weights = weights.transpose((1, 2, 0))\n",
    "\n",
    "quantized_image = padded * 10**EXPONENT\n",
    "quantized_depth_weights = depthWeights * 10**EXPONENT\n",
    "quantized_point_weights = pointWeights * 10**EXPONENT\n",
    "\n",
    "# q_input, q_weights, bias, actual, rem = DepthwiseConvInt(7, 7, 3, 3, 3, 1, 10**EXPONENT, quantized_image.round().astype(int), quantized_weights.round().astype(int), bias.astype(int))\n",
    "\n",
    "Input, DepthWeights, DepthBias, DepthRemainder, DepthOut, PointWeights, PointBias, remainder, out  = \\\n",
    "    SeparableConvTest(7, 7, 3, 3, 6, 3, 1, 1, quantized_image.round().astype(int), quantized_depth_weights.round().astype(int), quantized_point_weights.round().astype(int), depthBias.astype(int), pointBias.astype(int))\n",
    "\n",
    "# print(DepthOut)\n",
    "input_json_path = \"separableConv2D_input.json\"\n",
    "with open(input_json_path, \"w\") as input_file:\n",
    "    json.dump({\"in\": Input,\n",
    "               \"depthWeights\": DepthWeights,\n",
    "               \"depthBias\": DepthBias,\n",
    "               \"depthRemainder\": DepthRemainder,\n",
    "               \"depthOut\": DepthOut,\n",
    "               \n",
    "               \"pointWeights\": PointWeights,\n",
    "               \"pointBias\": PointBias,\n",
    "               \"pointRemainder\": remainder,\n",
    "               \"pointOut\": out,\n",
    "              },\n",
    "              input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "eab86d60-6b74-4e2f-8cdc-e130efac11e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '0', '0']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DepthBias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298522f0-5a9c-4975-9cad-b1a84e287084",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
